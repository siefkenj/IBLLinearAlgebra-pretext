
Suppose $\mathcal T$ is a linear transformation and $\vec v_1$ and $\vec v_2$ are eigenvectors
with eigenvalues $\lambda_1$ and $\lambda_2$. With this setup, for any $\vec a\in\Span\Set{\vec v_1,\vec v_2}$,
we can compute $\mathcal T(\vec a)$ with minimal effort.

Let's get specific. Define $\mathcal T:\R^2\to\R^2$
to be the linear transformation with matrix $M=\mat{1&2\\3&2}$. Let $\vec v_1=\mat{-1\\1}$ and $\vec v_2=\mat{2\\3}$,
and notice that $\vec v_1$ is an eigenvector for $\mathcal T$ with eigenvalue $-1$ and that $\vec v_2$ is an
eigenvector for $\mathcal T$ with eigenvalue $4$. Let $\vec a=\vec v_1+\vec v_2$.

Now,
\[
	\mathcal T(\vec a)=\mathcal T(\vec v_1+\vec v_2)=\mathcal T(\vec v_1)+\mathcal T(\vec v_2)=-\vec v_1+4\vec v_2.
\]

We didn't need to refer to the entries of $M$ to compute $\mathcal T(\vec a)$.

Exploring further, let $\mathcal V=\Set{\vec v_1,\vec v_2}$ and notice that $\mathcal V$ is a basis
for $\R^2$. By definition $[\vec a]_{\mathcal V}=\mat{1\\1}$, and so we just computed
\[
	\mathcal T\mat{1\\1}_{\mathcal V} = \mat{-1\\4}_{\mathcal V}.
\]
When represented in the $\mathcal V$ basis, computing $\mathcal T$ is easy. In general,
\[
	\mathcal T(\alpha\vec v_1+\beta\vec v_2)=\alpha\mathcal T(\vec v_1)+\beta\mathcal T(\vec v_2)=-\alpha\vec v_1+4\beta\vec v_2,
\]
and so
\[
	\mathcal T\mat{\alpha\\\beta}_{\mathcal V} = \mat{-\alpha\\4\beta}_{\mathcal V}.
\]
In other words, $\mathcal T$, when acting on vectors written in the $\mathcal V$ basis, just multiplies each coordinate
by an eigenvalue. This is enough information to determine the matrix for $\mathcal T$ in the $\mathcal V$ basis:
\[
	[\mathcal T]_{\mathcal V}=\mat{-1&0\\0&4}.
\]

The matrix representations $[\mathcal T]_{\mathcal E}=\mat{1&2\\3&2}$ and $[\mathcal T]_{\mathcal V}=\mat{-1&0\\0&4}$
are equally valid, but writing $\mathcal T$ in the $\mathcal V$ basis gives a very simple matrix!

\Heading{Diagonalization}

Recall that two matrices are similar\index{Matrix!similar matrices} if they represent the same transformation but in possibly different bases.
The process of \emph{diagonalizing} a matrix $A$ is that of finding a diagonal matrix that is similar to $A$,
and you can bet that this process is closely related to eigenvectors/values.

Let $\mathcal T:\R^n\to\R^n$ be a linear transformation and suppose that $\mathcal B=\Set{\vec b_1,\ldots,\vec b_n}$
is a basis so that
\[
	[\mathcal T]_{\mathcal B} = \matc{\alpha_1&0&\cdots &0\\0&\alpha_2&\cdots &0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&\alpha_n}
\]
is a diagonal matrix. This means that $\vec b_1,\ldots,\vec b_n$ are eigenvectors for $\mathcal T$! The proof goes as follows:
\[
	[\mathcal T]_{\mathcal B}[\vec b_1]_{\mathcal B} = 
	\matc{\alpha_1&0&\cdots &0\\0&\alpha_2&\cdots &0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&\alpha_n}
	\matc{1\\0\\\vdots\\0} = \matc{\alpha_1\\0\\\vdots\\0}=\alpha_1[\vec b_1]_{\mathcal B}=[\alpha_1\vec b_1]_{\mathcal B},
\]
and in general
\[
	[\mathcal T]_{\mathcal B}[\vec b_i]_{\mathcal B} = \alpha_i[\vec b_i]_{\mathcal B}=[\alpha_i\vec b_i]_{\mathcal B}.
\]
Therefore, for $i=1,\ldots,n$, we have
\[
	\mathcal T\vec b_i=\alpha_i\vec b_i.
\]
Since $\mathcal B$ is a basis, $\vec b_i\neq \vec 0$ for any $i$, and so each $\vec b_i$ is an eigenvector for $\mathcal T$ with
corresponding eigenvalue $\alpha_i$.

We've just shown that if a linear transformation $\mathcal T:\R^n\to\R^n$ can be represented by a diagonal matrix,
then there must be a basis for $\R^n$ consisting of eigenvectors for $\mathcal T$. The converse is also true.

Suppose again that $\mathcal T:\R^n\to\R^n$ is a linear transformation and that $\mathcal B=\Set{\vec b_1,\ldots,\vec b_n}$
is a basis of eigenvectors for $\mathcal T$ with corresponding eigenvalues $\alpha_1,\ldots,\alpha_n$. By definition,
\[
	\mathcal T(\vec b_i)=\alpha_i\vec b_i,
\]
and so
\[
	\mathcal T\matc{k_1\\k_2\\\vdots\\k_n}_{\mathcal B} = \matc{\alpha_1k_1\\\alpha_2k_2\\\vdots\\\alpha_nk_n}_{\mathcal B}
	\qquad\text{which is equivalent to}\qquad
	[\mathcal T]_{\mathcal B}\matc{k_1\\k_2\\\vdots\\k_n} = \matc{\alpha_1k_1\\\alpha_2k_2\\\vdots\\\alpha_nk_n}.
\]
The only matrix that does this is
\[
	[\mathcal T]_{\mathcal B} = \matc{\alpha_1&0&\cdots &0\\0&\alpha_2&\cdots &0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&\alpha_n},
\]
which is a diagonal matrix.

What we've shown is summarized by the following theorem.
\begin{theorem}
	A linear transformation $\mathcal T:\R^n\to\R^n$ can be represented by a diagonal matrix if and
	only if there exists a basis for $\R^n$ consisting of eigenvectors\index{Eigenvector} for $\mathcal T$. If
	$\mathcal B$ is such a basis, then $[\mathcal T]_{\mathcal B}$ is a diagonal matrix\index{Matrix!diagonal matrix}.
\end{theorem}

Now that we have a handle on representing a linear transformation by a diagonal matrix, let's tackle
the problem of diagonalizing a matrix itself.

\SavedDefinitionRender{Diagonalizable}

Suppose $A$ is an $n\times n$ matrix. $A$ induces some transformation $\mathcal T_A:\R^n\to\R^n$. By definition,
this means $A=[\mathcal T_A]_{\mathcal E}$. The matrix $B$ is similar to $A$ if there is some basis $\mathcal V$ so that
$B=[\mathcal T_A]_{\mathcal V}$. Using change-of-basis matrices, we see
\[
	A=\BasisChange{\mathcal V}{\mathcal E}[\mathcal T_A]_{\mathcal V}\BasisChange{\mathcal E}{\mathcal V}
	=\BasisChange{\mathcal V}{\mathcal E}B\BasisChange{\mathcal E}{\mathcal V}.
\]
In other words, $A$ and $B$ are similar if there is some invertible change-of-basis matrix $P$ so
\[
	A=PBP^{-1}.
\]
Based on our earlier discussion, $B$ will be a diagonal matrix if and only if $P$ is the change-of-basis\index{Basis!change-of-basis} matrix for a
basis of eigenvectors. In this case, we know $B$ will be the diagonal matrix with eigenvalues along the diagonal (in the proper order).

\begin{example}
	Let $A=\mat{1&2&5\\-11&14&5\\-3&2&9}$ be a matrix and notice that $\vec v_1=\mat{5\\5\\1}$, $\vec v_2=\mat{1\\1\\1}$,
	and $\vec v_3=\mat{1\\3\\1}$ are eigenvectors for $A$. Diagonalize $A$.

	First, we find the eigenvalues that correspond to the eigenvectors $\vec v_1, \vec v_2$, 
	and $\vec v_3$. Computing,
	\[
A\vec v_1=\matc{20\\20\\4}=4\vec v_1,\qquad A\vec v_2=\mat{8\\8\\8}=8\vec v_2,\qquad\text{and}\qquad
	A\vec v_3=\mat{12\\36\\12}=12\vec v_3, \]
	and so the eigenvalue corresponding to $\vec v_1$ is $4$, to $\vec v_2$ is $8$, 
	and to $\vec v_3$ is $12$.
	
	The change-of-basis matrix which converts from the $\Set{\vec v_1,\vec v_2,\vec v_3}$ to the standard basis is
	\[
		P = \mat{5&1&1\\5&1&3\\1&1&1},
	\]
	and
	\[
		P^{-1} = \mat{\frac{1}{4}&0&-\frac{1}{4}\\\frac{1}{4}&-\frac{1}{2}&\frac{5}{4}\\-\frac{1}{2}&\frac{1}{2}&0}.
	\]

	Define $D$ to be the $3\times 3$ matrix with the eigenvalues of $A$ along the diagonal (in the order, $4,8,12$). That is,
	the matrix $A$ written in the basis of eigenvectors is
	\[
		D = \mat{4&0&0\\0&8&0\\0&0&12}.
	\]
	We now know
	\[
		A = PDP^{-1}= \mat{5&1&1\\5&1&3\\1&1&1}\mat{4&0&0\\0&8&0\\0&0&12}
		\mat{\frac{1}{4}&0&-\frac{1}{4}\\\frac{1}{4}&-\frac{1}{2}&\frac{5}{4}\\-\frac{1}{2}&\frac{1}{2}&0},
	\]
	and that $D$ is the diagonalized form of $A$.
\end{example}

\Heading{Non-diagonalizable Matrices}

Is every matrix diagonalizable? Unfortunately the world is not that sweet. But, we have a tool to tell if a matrix is
diagonalizable\index{Diagonalizable}---checking to see if there is a basis of eigenvectors.

\begin{example}
	Is the matrix $R=\mat{0&-1\\1&0}$ diagonalizable?

	Computing, $\Char(R)=\lambda^2+1$ has no real roots. Therefore,
	$R$ has no real eigenvalues. Consequently, $R$ has no real eigenvectors,
	and so $R$ is not diagonalizable\footnote{ If we allow complex eigenvalues, then $R$ \emph{is}
	diagonalizable and is similar to the matrix $\mat{i&0\\0&-i}$. So, to be more precise, we
	might say $R$ is not \emph{real} diagonalizable.}.
\end{example}

\begin{example}
	Is the matrix $D=\mat{5&0\\0&5}$ diagonalizable?

	For every vector $\vec v\in \R^2$, we have $D\vec v=5\vec v$, and so every
	non-zero vector in $\R^2$ is an eigenvector for $D$. Thus, $\mathcal E=\Set{\xhat, \yhat}$
	is a basis of eigenvectors for $\R^2$, and so $D$ is diagonalizable\footnote{ Of course, every
	square matrix is similar to itself and $D$ is already diagonal, so of course it's diagonalizable.}.
\end{example}

\begin{example}
	Is the matrix $J=\mat{5&1\\0&5}$ diagonalizable?
	
	Computing, $\Char(J)=(5-\lambda)^2$ which has a double root at 5. Therefore, 5 is the
	only eigenvalue of $J$. The eigenvectors of $J$ all lie in 
	\[
		\Null(J-5I)=\Span\Set*{\mat{1\\0}}.
	\]
	Since this is a one dimensional space, there is no basis for $\R^2$ consisting
	of eigenvectors for $J$. Therefore,  $J$ is not diagonalizable.
\end{example}

\begin{example}
	Is the matrix $K=\mat{5&1\\0&2}$ diagonalizable?
	
	Computing, $\Char(K)=(5-\lambda)(2-\lambda)$ which has roots at 5 and 2. Therefore, 5 and 2 are
	the eigenvalues of $K$. The eigenvectors of $K$ lie in one of
	\[
		\Null(K-5I) = \Span\Set*{\mat{1\\0}}\qquad\text{or}\qquad\Null(K-2I) = \Span\Set*{\mat{-1\\3}}.
	\]
	Picking one eigenvector from each null space, we have that 
	$\Set*{\mat{1\\0},\mat{-1\\3}}$ is a basis for $\R^2$ consisting of eigenvectors of $K$. Thus, $K$ is diagonalizable.
\end{example}

\begin{emphbox}[Takeaway]
	Not all matrices are diagonalizable, but you can check if an $n\times n$
	matrix is diagonalizable by determining whether there is a basis of eigenvectors for $\R^n$.
\end{emphbox}

\Heading{Geometric and Algebraic Multiplicities}

When analyzing linear transformations or matrices, we're often interested in
studying the subspaces where vectors are stretched by only one eigenvalue. These 
are called the \emph{eigenspaces}.

\SavedDefinitionRender{Eigenspace}

Now is the time when linear algebra and regular algebra (the solving of non-linear equations)
combine. We know, every root of the characteristic polynomial of a matrix gives an eigenvalue
for that matrix. Since the degree of the characteristic polynomial of an $n\times n$ matrix
is always $n$, the fundamental theorem of algebra tells us exactly how many roots to expect.

Recall that the \emph{multiplicity} of a root of a polynomial is the power of that root
in the factored polynomial. So, for example $p(x)=(4-x)^3(5-x)$ has a root of $4$ with multiplicity
$3$ and a root of $5$ with multiplicity $1$.

\begin{example}
	Let $R=\mat{0&-1\\1&0}$ and find the geometric and algebraic multiplicity of each eigenvalue of $R$.
	
	Computing, $\Char(R)=\lambda^2+1$ which has no real roots. Therefore,
	$R$ has no real eigenvalues.\footnote{ If we allow complex eigenvalues, then the eigenvalues
	$i$ and $-i$ both have geometric and algebraic multiplicity of 1.}
\end{example}

\begin{example}
	Let $D=\mat{5&0\\0&5}$ and find the geometric and algebraic multiplicity of each eigenvalue of $D$.
	
	Computing, $\Char(D)=(5-\lambda)^2$, so $5$ is an eigenvalue of $D$ with algebraic multiplicity $2$.
	The eigenspace of $D$ corresponding to 5 is $\R^2$. Thus, the geometric multiplicity of $5$ is $2$.
\end{example}

\begin{example}
	Let $J=\mat{5&1\\0&5}$ and find the geometric and algebraic multiplicity of each eigenvalue of $J$.
	
	Computing, $\Char(J)=(5-\lambda)^2$, so $5$ is an eigenvalue of $J$ with algebraic multiplicity $2$.
	The eigenspace of $J$ corresponding to $5$ is $\Span\Set*{\mat{1\\0}}$. Thus, the geometric 
	multiplicity of $5$ is $1$.
\end{example}

\begin{example}
	Let $K=\mat{5&1\\0&2}$ and find the geometric and algebraic multiplicity of each eigenvalue of $K$.
	
	Computing, $\Char(K)=(5-\lambda)(2-\lambda)$, so $5$ and $2$ are eigenvalues of $K$, both with algebraic multiplicity $1$.
	The eigenspace of $K$ corresponding to $5$ is $\Span\Set*{\mat{1\\0}}$ and the eigenspace
	corresponding to $2$ is $\Span\Set*{\mat{-1\\3}}$. Thus, both $5$ and $2$ have a geometric
	multiplicity of $1$.
\end{example}


Consider the following two theorems.

\begin{theorem}[Fundamental Theorem of Algebra]
	Let $p$ be a polynomial of degree $n$. Then, if complex roots are allowed,
	the sum of the multiplicities of the roots of $p$ is $n$.
\end{theorem}

\begin{theorem}
	Let $\lambda$ be an eigenvalue of the matrix $A$. Then
	\[
		\text{geometric mult}(\lambda)\leq \text{algebraic mult}(\lambda).
	\]
\end{theorem}

We can now deduce the following.
\begin{theorem}
	An $n\times n$ matrix $A$ is diagonalizable if and only if the sum of its geometric multiplicities
	is equal to $n$. Further, provided complex eigenvalues are permitted, $A$ is diagonalizable if and
	only if its geometric multiplicities are equal to its corresponding algebraic multiplicities.
\end{theorem}
\begin{proof}
	Let $A$ be an $n\times n$ matrix with eigenvalues $\lambda_1,\ldots,\lambda_k$. Let $E_1,\ldots,E_k$
	be bases for the eigenspaces corresponding to $\lambda_1,\ldots,\lambda_k$. We will start by showing $E=E_1\cup\cdots\cup E_k$
	is a linearly independent set using the following two lemmas.

	\emph{\color{myorange}No New Eigenvalue Lemma.} Suppose that $\vec v_1,\ldots,\vec v_k$ are linearly independent eigenvectors
	of a matrix $A$, and let $\lambda_1,\ldots,\lambda_k$ be the  corresponding eigenvalues. Then, any eigenvector for $A$ contained in
	$\Span\Set{\vec v_1,\ldots,\vec v_k}$ must have one of $\lambda_1,\ldots,\lambda_k$ as its eigenvalue.

	The proof goes as follows. Suppose $\vec v=\sum_{i\leq k}\alpha_i\vec v_i$ is an eigenvector for $A$ with
	eigenvalue $\lambda$. We now compute $A\vec v$ in two different ways: once by using the fact that $\vec v$ is an eigenvector, and
	again by using the fact that $\vec v$ is a linear combination of other eigenvectors. Observe
	\[
		A\vec v=\lambda \vec v=\lambda\left(\sum_{i\leq k}\alpha_i\vec v_i\right)
		=\sum_{i\leq k}\alpha_i\lambda\vec v_i
	\]
	and
	\[
		A\vec v=A\left(\sum_{i\leq k}\alpha_i\vec v_i\right)
		=\sum_{i\leq k}\alpha_i A\vec v_i
		=\sum_{i\leq k}\alpha_i\lambda_i\vec v_i.
	\]
	We now have
	\[
		\vec 0=A\vec v-A\vec v = 
		\sum_{i\leq k}\alpha_i\lambda\vec v_i
		-\sum_{i\leq k}\alpha_i\lambda_i\vec v_i
		=\sum_{i\leq k}\alpha_i(\lambda-\lambda_i)\vec v_i.
	\]
	Because $\vec v_1,\ldots,\vec v_k$ are linearly independent, we know $\alpha_i(\lambda-\lambda_i)=0$ for all $i\leq k$.
	Further, because $\vec v$ is non-zero (it's an eigenvector),
	we know at least one $\alpha_i$ is non-zero. Therefore $\lambda-\lambda_i=0$ for at least one
	$i$. In other words, $\lambda=\lambda_i$ for at least one $i$, which is what we set out to show\footnote{ You may notice
	that we've proved something stronger than we needed: if an eigenvector is a linear combination of linearly independent eigenvectors,
	the only non-zero coefficients of that linear combination must belong to eigenvectors with the same eigenvalue.}.

	\emph{\color{myorange}Basis Extension Lemma.} Let $P=\Set{\vec p_1,\ldots,\vec p_a}$ and 
	$Q=\Set{\vec q_1,\ldots,\vec q_b}$ be linearly independent sets, and suppose $P\cup\Set{\vec q}$
	is linearly independent for all non-zero $\vec q\in\Span Q$. Then $P\cup Q$ is linearly independent.

	To show this, suppose $\vec 0=\alpha_1\vec p_1+\cdots+\alpha_a\vec p_a+\beta_1\vec q_1+\cdots+\beta_b\vec q_b$ is a linear
	combination of vectors in $P\cup Q$. Let $\vec q=\beta_1\vec q_1+\cdots+\beta_b\vec q_b$. First, note that $\vec q$ must
	be the zero vector. If not, $\vec 0=\alpha_1\vec p_1+\cdots+\alpha_a\vec p_a+\vec q$ is a non-trivial linear combination
	of vectors in $P\cup\Set{\vec q}$, which contradicts the assumption that $P\cup\Set{\vec q}$ is linearly independent.
	Since we've established $\vec 0=\vec q=\beta_1\vec q_1+\cdots+\beta_b\vec q_b$,
	we conclude $\beta_1=\cdots=\beta_b=0$ because $Q$ is linearly independent. It follows that since $\vec 0=\alpha_1\vec p_1+\cdots+\alpha_a\vec p_a+\vec q
	=\alpha_1\vec p_1+\cdots+\alpha_a\vec p_a+\vec 0$, we must have that $\alpha_1=\cdots=\alpha_a=0$ because $P$ is linearly independent.
	This shows that the only way to express $\vec 0$ as a linear combination of vectors in $P\cup Q$ is as the trivial linear combination,
	and so $P\cup Q$ is linearly independent.

	\medskip
	Now we can put our lemmas to good use. We will use induction to show that $E=E_1\cup\cdots\cup E_k$ is linearly independent.
	By assumption $E_1$ is linearly independent. Now, suppose $U=E_1\cup\cdots\cup E_j$ is linearly independent. By construction,
	every non-zero vector $\vec v\in\Span E_{j+1}$ is an eigenvector for $A$ with eigenvalue $\lambda_{j+1}$. Therefore, 
	since $\lambda_{j+1}\neq \lambda_i$ for $1\leq i\leq j$, we may apply
	the \emph{No New Eigenvalue Lemma} to see that $\vec v\notin\Span U$. It follows that $U\cup\Set{\vec v}$ is linearly independent.
	Since $E_{j+1}$ is itself linearly independent, we may now apply the \emph{Basis Extension Lemma} to deduce that $U\cup E_{j+1}$
	is linearly independent. This shows that $E=E_1\cup\cdots\cup E_k$ is linearly independent.

	To conclude notice that by construction, $\text{geometric mult}(\lambda_i)=\abs{E_i}$. Since $E=E_1\cup\cdots\cup E_k$ is linearly independent,
	the $E_i$'s must be disjoint and so $\sum\text{geometric mult}(\lambda_i)=\sum \abs{E_i}=\abs{E}$. If $\sum\text{geometric mult}(\lambda_i)=n$,
	then $E\subseteq\R^n$ is a linearly independent set of $n$ vectors and so is a basis for $\R^n$.
	Finally, because we have a basis for $\R^n$ consisting of eigenvectors
	for $A$, we know $A$ is diagonalizable.


	Conversely, if there is
	a basis $E$ for $\R^n$ consisting of eigenvectors, we must have a linearly independent set of $n$ eigenvectors. Grouping
	these eigenvectors by eigenvalue, an application of the \emph{No New Eigenvalue Lemma} shows that each group
	must actually be a basis for its eigenspace. Thus, the sum of the geometric
	multiplicities must be $n$.

	Finally, if complex eigenvalues are allowed, the algebraic multiplicities sum to $n$. Since the algebraic multiplicities
	bound the geometric multiplicities, the only way for the geometric multiplicities to sum to $n$ is if
	corresponding geometric and algebraic multiplicities are equal.


\end{proof}
