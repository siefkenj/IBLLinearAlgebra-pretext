\begin{exercises}
	\begin{problist}
		\prob For each description below, if possible, create a matrix 
		matching the description. Otherwise, explain why such a matrix doesn't
		exist.
		\begin{enumerate}
			\item A $2\times 2$ diagonal matrix whose entries sum to $-1$.
			\item A $2\times 2$ symmetric matrix whose entries sum to $-1$.
			\item A $4\times 2$ symmetric matrix whose entries sum to $-1$.
			\item A $3\times 3$ skew-symmetric matrix whose entries sum to $-1$.
			\item A $1\times 4$ matrix $A=[a_{ij}]$ whose entries satisfy $a_{ij}=\sqrt{i+j}$.
		\end{enumerate}
		\begin{solution}
			Note: there are many matrices possible.
			\begin{enumerate}
				\item $\mat{1&0\\0&-2}$
				\item $\mat{1&1\\1&-4}$ or $\mat{1&0\\0&-2}$
				\item Impossible. A symmetric matrix must be square.
				\item Impossible. Let $A=[a_{ij}]$ be a skew symmetric matrix. By definition, 
					$a_{ij}=-a_{ji}$. In particular, the diagonal entries satisfy $a_{kk}=-a_{kk}$
					and so must be zero. For every other entry $a_{ij}$ with $i\neq j$, there exists
					a corresponding entry $-a_{ji}$. Therefore the sum of all entries must be zero.
				\item $\mat{\sqrt{2}&\sqrt{3}&2&\sqrt{5}}$

			\end{enumerate}
		\end{solution}
		\prob Consider the following matrices
		$A=\mat{1 & 0 & 0 \\ 1 & 0 & 2 \\ 1 & 6 & 5}$, $B=\mat{2 & 1 & 1 \\ 0 & 2 & 0 \\ 0 & 0 & 3}$,
		$C=\mat{1\\-1\\2}$, and $D=\mat{0&-2&1}$. For each of the following, (i) determine if the operation
		is defined, and (ii) compute the result using both the column picture
		and row picture of multiplication (if applicable).
		\begin{enumerate}
			\item $AC$
			\item $2A+B$
			\item $A-B$
			\item $CA$
			\item $AB$
			\item $BA$
			\item $DC$
			\item $CD$
		\end{enumerate}
		\begin{solution}
			\begin{enumerate}
				\item $\mat{1\\5\\5}$
				\item $\mat{4&1&1\\2&2&4\\2&12&13}$
				\item $\mat{-1&-1&-1\\1&-2&2\\1&6&2}$
				\item Not defined
				\item $\mat{2&1&1\\2&1&7\\2&13&16}$
				\item $\mat{4&6&7\\2&0&4\\3&18&15}$
				\item $\mat{4}$
				\item $\mat{0&-2&1\\0&2&-1\\0&-4&2}$
			\end{enumerate}
		\end{solution}
		\prob In general, matrix multiplication is non-commutative. However, some types of matrices are special.

		Let $A$ and $B$ be $2\times 2$ diagonal matrices and let $X$ and $Y$ be $n\times n$ diagonal matrices.
		\begin{enumerate}
			\item Show by direct computation that $AB=BA$.
			\item Show that both $XY$ and $YX$ also diagonal matrices.
			\item Is it true that $XY=YX$ no matter $n$? Explain.
		\end{enumerate}

		\prob Classify the following statements as true or false.
		\begin{enumerate}
			\item A matrix in reduced row echelon form is an upper triangular matrix.
			\item A diagonal matrix is in reduced row echelon form.
			\item Every zero matrix is also square.
			\item A zero matrix is neither upper or lower triangular.
			\item A matrix that is both upper triangular and lower triangular must be diagonal.
			\item Using row operations every lower triangular matrix can be converted into an upper triangular matrix.
			\item The product of two lower triangular matrices is a lower triangular matrix (provided the product
				is defined).
		\end{enumerate}
		\begin{solution}
			\begin{enumerate}
				\item True
				\item False. Consider $\mat{0&0\\0&1}$.
				\item False. Zero matrices can be of any size.
				\item False. All zero matrices are both upper and lower triangular.
				\item False. Diagonal matrices must be square; upper/lower triangular matrices can be of any size.
				\item True
				\item True
			\end{enumerate}
		\end{solution}

		\prob Let $R$ be a $1\times n$ matrix and let $C$ be an $n\times 1$ matrix.
		\begin{enumerate}
			\item Is the product $RC$ defined? If so, what is its shape?
			\item Is the product $CR$ defined? If so, what is its shape?
			\item Let $\vec r$ be the (only) row vector in $R$ and let $\vec c$ be
				the (only) column vector in $C$. Are $\vec r\cdot \vec c$ and $RC$ the
				same? Explain.
			\item Let $\vec x,\vec y\in \R^n$ and let $R_{\vec x}$ be the $1\times n$ matrix with $\vec x$
				as a row vector and let $C_{\vec y}$ be the $n\times 1$ matrix with $\vec y$ as a
				column vector. The \emph{inner product}\index{Inner product} of $\vec x$ and $\vec y$ is defined
				to be $R_{\vec x}C_{\vec y}$. The \emph{outer product}\index{Outer product} of $\vec x$ and $\vec y$ is
				defined to be $C_{\vec y}R_{\vec x}$.
				\begin{enumerate}
					\item How does the inner product of $\vec x$ and $\vec y$ relate to the dot product of $\vec x$ and $\vec y$?
					\item Let $Q$ be the outer product of $\vec x$ and $\vec y$. What does the reduced row echelon form of $Q$
						look like?
					\item Let  $Q$ be the outer product of $\vec x$ and $\vec y$. Show that the columns of $Q$ are always linearly
						dependent when $n\geq 2$.
				\end{enumerate}
		\end{enumerate}
		\begin{solution}
			\begin{enumerate}
				\item Yes. $1\times 1$
				\item Yes. $n\times n$
				\item They are related but not the same. $\vec r\cdot \vec c$ is a scalar
					and $RC$ is a $1\times 1$ matrix.
				\item
					\begin{enumerate}
						\item The inner product of $\vec x$ and $\vec y$ is $[\vec x\cdot \vec y]$.
							That is, it is the $1\times 1$ matrix with entry $\vec x\cdot \vec y$.
						\item The reduced row echelon form of $Q$ looks like a (possibly) non-zero
							row followed by rows of zeros.
						\item Let $y_1,\ldots, y_n\in \R$ be the entries in $\vec y$. Then
							the columns of $Q$ are $y_1\vec x,\ldots, y_n\vec x$. These columns
							are all scalar multiples of the same vector, $\vec x$, and so are
							linearly dependent.
					\end{enumerate}
			\end{enumerate}
		\end{solution}

		\prob A $3\times 3$ matrix is called a \emph{Heisenberg matrix}\index{Heisenberg matrix} if it takes the form
			$\mat{1&a&c\\0&1&b\\0&0&1}$ for some $a,b,c\in\R$.
		\begin{enumerate}
			\item Show that if $A$ and $B$ are Heisenberg matrices, then so are $AB$ and $BA$.
			\item If $A$ and $B$ are Heisenberg matrices, is it always the case that $AB=BA$? Give a proof
				or a counter example.
			\item Let $X=\mat{1&a&c\\0&1&b\\0&0&1}$ and let $Y=\mat{1&-1&ab-c\\0&1&-b\\0&0&1}$. Show that $XY=I_{3\times 3}$.
		\end{enumerate}
		\begin{solution}
			\begin{enumerate}
				\item Multiplying \[
						\mat{1&a&b\\0&1&c\\0&0&1}\mat{1&x&y\\0&1&z\\0&0&1}
						=\mat{1&a+x&c+ay+z\\0&1&b+y\\0&0&1}
				\]
					and so two Heisenberg matrices aways multiply together to
					form another Heisenberg matrix.
				\item It may be that $AB\neq BA$. For example
				\[
					\mat{1&2&0\\0&1&1\\0&0&1}
					\mat{1&1&0\\0&1&2\\0&0&1}
					\neq
					\mat{1&1&0\\0&1&2\\0&0&1}
					\mat{1&2&0\\0&1&1\\0&0&1}.
				\]

				\item By multiplying out, we see $XY=I$ (and $YX=I$).
			\end{enumerate}
		\end{solution}


		\prob Let $X$ be a matrix of the form $\mat{a&-b\\b&a}$ for some $a,b\in\R$.
		\begin{enumerate}
			\item Show that $X^2$ has the same form as $X$.
			\item Is there a solution to the matrix equation $X^2=I_{2\times 2}$? If so, how many?
			\item Is there a solution to the matrix equation $X^2=-I_{2\times 2}$? If so, how many?
			\item Let $Y$ be an arbitrary $2\times 2$ matrix. How many solutions are there to the equation
				$Y^2=I_{2\times 2}$? 
			\item Do you agree with the statement ``every positive real number has exactly
				two square roots''? Do you agree with the statement ``every diagonal matrix with positive entries on the diagonal
				has exactly two square roots''? Explain.
		\end{enumerate}
		\begin{solution}
			\begin{enumerate}
				\item Multiplying out, we see
					\[
						\mat{a&-b\\b&a}\mat{x&-y\\y&x}=\mat{ax-by&-(ay+bx)\\ay+bx&ax-by}
					\]
				has the required form.
				\item Multiplying, we see
					\[
						X^2=\mat{a&-b\\b&a}^2=\mat{a^2-b^2&-2ab\\2ab&a^2-b^2}=\mat{1&0\\0&1}
					\]
					implies that $a=0$ or $b=0$. If $a=0$, then $-b^2=1$, which is impossible. Therefore
					$b=0$. This means $a^2=1$ which has solutions $a=\pm 1$. Therefore there
					are exactly two solutions to $X^2=I$.
				\item Multiplying, we see
					\[
						X^2=\mat{a&-b\\b&a}^2=\mat{a^2-b^2&-2ab\\2ab&a^2-b^2}=\mat{-1&0\\0&-1}
					\]
					implies that $a=0$ or $b=0$. If $a=b$, then $a^2=-1$, which is impossible. Therefore
					$a=0$. This means $-b^2=-1$ which has solutions $b=\pm 1$. Therefore there
					are exactly two solutions to $X^2=-I$.
				\item There are infinitely many solutions to $Y^2=I$. For example $\mat{0&t\\1/t&0}^2=I$ for
					any non-zero $t$.
				\item Yes to the first, no to the second. Matrices are more general than numbers!
			\end{enumerate}
		\end{solution}
	\end{problist}
\end{exercises} 
